{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pyenv local 3.10.16 #pyflink doesnt work with python 3.12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Redpanda version\n",
    "\n",
    "Now let's find out the version of redpandas. \n",
    "\n",
    "For that, check the output of the command `rpk help` _inside the container_. The name of the container is `redpanda-1`.\n",
    "\n",
    "Find out what you need to execute based on the `help` output.\n",
    "\n",
    "What's the version, based on the output of the command you executed? (copy the entire version)\n",
    "\n",
    "Answer:\n",
    "```bash\n",
    "docker exec -it redpanda-1 bash\n",
    "\n",
    "redpanda@9e8c317dcbb1:/$ rpk --version\n",
    "\n",
    "# rpk version v24.2.18 (rev f9a22d4430)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Creating a topic\n",
    "\n",
    "Before we can send data to the redpanda server, we\n",
    "need to create a topic. We do it also with the `rpk`\n",
    "command we used previously for figuring out the version of \n",
    "redpandas.\n",
    "\n",
    "Read the output of `help` and based on it, create a topic with name `green-trips` \n",
    "\n",
    "What's the output of the command for creating a topic? Include the entire output in your answer.\n",
    "\n",
    "Answer\n",
    "\n",
    "```bash\n",
    "redpanda@9e8c317dcbb1:/$ rpk help topic create\n",
    "\n",
    "redpanda@9e8c317dcbb1:/$ rpk topic create green-trips\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```text\n",
    "TOPIC        STATUS\n",
    "green-trips  OK\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Connecting to the Kafka server\n",
    "\n",
    "We need to make sure we can connect to the server, so\n",
    "later we can send some data to its topics\n",
    "\n",
    "First, let's install the kafka connector (up to you if you\n",
    "want to have a separate virtual environment for that)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (2.0.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start a jupyter notebook in your solution folder or\n",
    "create a script\n",
    "\n",
    "Let's try to connect to our server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided that you can connect to the server, what's the output\n",
    "of the last command?\n",
    "\n",
    "```text\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Sending the Trip Data\n",
    "\n",
    "Now we need to send the data to the `green-trips` topic\n",
    "\n",
    "Read the data, and keep only these columns:\n",
    "\n",
    "* `'lpep_pickup_datetime',`\n",
    "* `'lpep_dropoff_datetime',`\n",
    "* `'PULocationID',`\n",
    "* `'DOLocationID',`\n",
    "* `'passenger_count',`\n",
    "* `'trip_distance',`\n",
    "* `'tip_amount'`\n",
    "\n",
    "Now send all the data using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting numpy>=1.22.4\n",
      "  Downloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.3 pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'green-trips'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 7.9M\n",
      "-rw-r--r-- 1 root root 7.9M Jul 14  2022 green_tripdata_2019-10.csv.gz\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ../../../week_5/codes/data/raw/green/2019/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../week_5/codes/data/raw/green/2019/10/green_tripdata_2019-10.csv.gz', low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:26:02</td>\n",
       "      <td>2019-10-01 00:39:58</td>\n",
       "      <td>112</td>\n",
       "      <td>196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:18:11</td>\n",
       "      <td>2019-10-01 00:22:38</td>\n",
       "      <td>43</td>\n",
       "      <td>263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:09:31</td>\n",
       "      <td>2019-10-01 00:24:47</td>\n",
       "      <td>255</td>\n",
       "      <td>228</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:37:40</td>\n",
       "      <td>2019-10-01 00:41:49</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 00:08:13</td>\n",
       "      <td>2019-10-01 00:17:56</td>\n",
       "      <td>97</td>\n",
       "      <td>188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lpep_pickup_datetime lpep_dropoff_datetime  PULocationID  DOLocationID  \\\n",
       "0  2019-10-01 00:26:02   2019-10-01 00:39:58           112           196   \n",
       "1  2019-10-01 00:18:11   2019-10-01 00:22:38            43           263   \n",
       "2  2019-10-01 00:09:31   2019-10-01 00:24:47           255           228   \n",
       "3  2019-10-01 00:37:40   2019-10-01 00:41:49           181           181   \n",
       "4  2019-10-01 00:08:13   2019-10-01 00:17:56            97           188   \n",
       "\n",
       "   passenger_count  trip_distance  tip_amount  \n",
       "0              1.0           5.88        0.00  \n",
       "1              1.0           0.80        0.00  \n",
       "2              2.0           7.50        0.00  \n",
       "3              1.0           0.90        0.00  \n",
       "4              1.0           2.52        2.26  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = [\n",
    "    'lpep_pickup_datetime',\n",
    "    'lpep_dropoff_datetime',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'tip_amount'\n",
    "]\n",
    "df = df[selected_columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df.to_dict(orient='records')\n",
    "\n",
    "# messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.kafka.KafkaProducer at 0x7fca723620b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row (`message`) in the dataset. In this case, `message`\n",
    "is a dictionary.\n",
    "\n",
    "After sending all the messages, flush the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `from time import time` to see the total time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much time did it take to send the entire dataset and flush? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to send and flush: 45.66 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "for message in messages:\n",
    "    producer.send(topic_name, value=message)\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time()\n",
    "took = t1 - t0\n",
    "print(f\"Time taken to send and flush: {took:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Build a Sessionization Window (2 points)\n",
    "Now we have the data in the Kafka stream. It's time to process it.\n",
    "\n",
    "* Copy `aggregation_job.py` and rename it to `session_job.py`\n",
    "* Have it read from `green-trips` fixing the schema\n",
    "* Use a [session window](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/) with a gap of 5 minutes\n",
    "* Use `lpep_dropoff_datetime` time as your watermark with a 5 second tolerance\n",
    "* Which pickup and drop off locations have the longest unbroken streak of taxi trips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache-flink\n",
      "  Downloading apache-flink-1.20.1.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ruamel.yaml>=0.18.4\n",
      "  Downloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from apache-flink) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from apache-flink) (2.32.3)\n",
      "Collecting protobuf>=3.19.0\n",
      "  Downloading protobuf-6.30.0-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting apache-beam<2.49.0,>=2.43.0\n",
      "  Using cached apache_beam-2.48.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.3 MB)\n",
      "Collecting httplib2>=0.19.0\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from apache-flink) (2.2.3)\n",
      "Collecting pemja==0.4.1\n",
      "  Downloading pemja-0.4.1-cp310-cp310-manylinux1_x86_64.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from apache-flink) (2.9.0.post0)\n",
      "Collecting pyarrow>=5.0.0\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "Collecting fastavro!=1.8.0,>=1.1.0\n",
      "  Using cached fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting cloudpickle>=2.2.0\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting apache-flink-libraries<1.20.2,>=1.20.1\n",
      "  Downloading apache-flink-libraries-1.20.1.tar.gz (231.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.1/231.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting avro-python3!=1.9.2,>=1.8.1\n",
      "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?2done\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from apache-flink) (2025.1)\n",
      "Collecting find-libpython\n",
      "  Downloading find_libpython-0.4.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting orjson<4.0\n",
      "  Using cached orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Collecting numpy>=1.22.4\n",
      "  Using cached numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting protobuf>=3.19.0\n",
      "  Using cached protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "Collecting zstandard<1,>=0.18.0\n",
      "  Using cached zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from apache-beam<2.49.0,>=2.43.0->apache-flink) (4.12.2)\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Using cached dill-0.3.1.1.tar.gz (151 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting grpcio!=1.48.0,<2,>=1.33.1\n",
      "  Using cached grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Collecting proto-plus<2,>=1.7.1\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Using cached hdfs-2.7.3.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cloudpickle>=2.2.0\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Using cached crcmod-1.7.tar.gz (89 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fasteners<1.0,>=0.3\n",
      "  Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Collecting pyarrow>=5.0.0\n",
      "  Using cached pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "Collecting pymongo<5.0.0,>=3.8.0\n",
      "  Using cached pymongo-4.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting regex>=2020.6.8\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Collecting objsize<0.7.0,>=0.6.1\n",
      "  Using cached objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from pandas>=1.3.0->apache-flink) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from python-dateutil<3,>=2.8.0->apache-flink) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from requests>=2.26.0->apache-flink) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from requests>=2.26.0->apache-flink) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from requests>=2.26.0->apache-flink) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages (from requests>=2.26.0->apache-flink) (3.4.1)\n",
      "Collecting ruamel.yaml.clib>=0.2.7\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (722 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.2/722.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting docopt\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0\n",
      "  Using cached dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Building wheels for collected packages: apache-flink\n",
      "  Building wheel for apache-flink (pyprojdone\n",
      "\u001b[?25h  Created wheel for apache-flink: filename=apache_flink-1.20.1-cp310-cp310-linux_x86_64.whl size=6081620 sha256=eb67ed027bd0fd259323b8b57a5ef5c491de063187f539c7d1adfae39a2495bb\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/fb/de/59899be1e28bbf423094be8c928c2e160aa1feea66b1d74c5d\n",
      "Successfully built apache-flink\n",
      "Installing collected packages: py4j, find-libpython, docopt, crcmod, zstandard, ruamel.yaml.clib, regex, pyparsing, protobuf, pemja, orjson, objsize, numpy, grpcio, fasteners, fastavro, dnspython, dill, cloudpickle, avro-python3, apache-flink-libraries, ruamel.yaml, pymongo, pydot, pyarrow, proto-plus, httplib2, hdfs, apache-beam, apache-flink\n",
      "\u001b[33m  DEPRECATION: docopt is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "  Running setup.py install for docopt ... \u001b[?25ldone\n",
      "\u001b[33m  DEPRECATION: crcmod is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[?25h  Running setup.py install for crcmod ... \u001b[?2done\n",
      "\u001b[?25h  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "\u001b[33m  DEPRECATION: dill is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "  Running setup.py install for dill ... \u001b[?25ldone\n",
      "\u001b[33m  DEPRECATION: avro-python3 is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[?25h  Running setup.py install for avro-python3 ... \u001b[?25done\n",
      "\u001b[33m  DEPRECATION: apache-flink-libraries is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[?25h  Running setup.py install for apache-flink-libraries ... \u001b[?done\n",
      "\u001b[33m  DEPRECATION: hdfs is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[?25h  Running setup.py install for hdfs ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed apache-beam-2.48.0 apache-flink-1.20.1 apache-flink-libraries-1.20.1 avro-python3-1.10.2 cloudpickle-2.2.1 crcmod-1.7 dill-0.3.1.1 dnspython-2.7.0 docopt-0.6.2 fastavro-1.10.0 fasteners-0.19 find-libpython-0.4.0 grpcio-1.71.0 hdfs-2.7.3 httplib2-0.22.0 numpy-1.24.4 objsize-0.6.1 orjson-3.10.15 pemja-0.4.1 proto-plus-1.26.1 protobuf-4.23.4 py4j-0.10.9.7 pyarrow-11.0.0 pydot-1.4.2 pymongo-4.11.2 pyparsing-3.2.1 regex-2024.11.6 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 zstandard-0.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install apache-flink\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/homework/session_job.py\", line 77, in <module>\n",
      "    sessionize_trips()\n",
      "  File \"/root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/homework/session_job.py\", line 62, in sessionize_trips\n",
      "    t_env.execute_sql(f\"\"\"\n",
      "  File \"/root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages/pyflink/table/table_environment.py\", line 837, in execute_sql\n",
      "    return TableResult(self._j_tenv.executeSql(stmt))\n",
      "  File \"/root/spark/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "  File \"/root/app/practice/Zoomcamp-data-engineering/week_6/pyflink_by_zack/streaming-venv/lib/python3.10/site-packages/pyflink/util/exceptions.py\", line 146, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/root/spark/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o20.executeSql.\n",
      ": org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.green_trips'.\n",
      "\n",
      "Table options are:\n",
      "\n",
      "'connector'='kafka'\n",
      "'format'='json'\n",
      "'properties.bootstrap.servers'='redpanda-1:29092'\n",
      "'scan.startup.mode'='earliest-offset'\n",
      "'topic'='green-trips'\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:235)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:260)\n",
      "\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:175)\n",
      "\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:115)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:4034)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2904)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2464)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2378)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2323)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:730)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:716)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3880)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryOrInList(SqlToRelConverter.java:1912)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertExists(SqlToRelConverter.java:1895)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.substituteSubQuery(SqlToRelConverter.java:1421)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.replaceSubQueries(SqlToRelConverter.java:1161)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertCollectionTable(SqlToRelConverter.java:2928)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2511)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2378)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2323)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:730)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:716)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3880)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:620)\n",
      "\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:233)\n",
      "\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:208)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeConvertContext.toRelRoot(SqlNodeConvertContext.java:77)\n",
      "\tat org.apache.flink.table.planner.operations.converters.SqlQueryConverter.convertSqlNode(SqlQueryConverter.java:48)\n",
      "\tat org.apache.flink.table.planner.operations.converters.SqlNodeConverters.convertSqlNode(SqlNodeConverters.java:85)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:271)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNodeOrFail(SqlNodeToOperationConversion.java:387)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertSqlInsert(SqlNodeToOperationConversion.java:774)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:350)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:261)\n",
      "\tat org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)\n",
      "\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:728)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option: 'connector'='kafka'\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:814)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:788)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:231)\n",
      "\t... 46 more\n",
      "Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.\n",
      "\n",
      "Available factory identifiers are:\n",
      "\n",
      "blackhole\n",
      "datagen\n",
      "filesystem\n",
      "print\n",
      "python-input-format\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:624)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:810)\n",
      "\t... 48 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python session_job.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
